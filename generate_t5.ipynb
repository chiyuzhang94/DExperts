{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Union, List\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, modeling_utils, GPT2PreTrainedModel, T5ForConditionalGeneration, T5TokenizerFast\n",
    "from generation.gpt2_generation import GPT2Generation\n",
    "\n",
    "from utils import utils\n",
    "from utils.generation_utils import top_k_top_p_filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = int(10000)  # Hardcoded max length to avoid infinite loop\n",
    "\n",
    "class DExpertsGeneration: \n",
    "    STOP_TOKEN = \"</s>\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        base_model: Union[str, Path, T5ForConditionalGeneration],\n",
    "        antiexpert_model: Union[str, Path, T5ForConditionalGeneration] = None,\n",
    "        expert_model: Union[str, Path, T5ForConditionalGeneration] = None,\n",
    "        tokenizer: str = 'gpt2', \n",
    "        seed: int = 42,\n",
    "        expert_prefix: str = None,\n",
    "        antiexpert_prefix: str = None\n",
    "        ):\n",
    "        # Set up device\n",
    "        self.expert_prefix = expert_prefix\n",
    "        self.antiexpert_prefix = antiexpert_prefix\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        n_gpu = torch.cuda.device_count()\n",
    "        utils.set_seed(seed, n_gpu)\n",
    "\n",
    "        self.base_model = T5ForConditionalGeneration.from_pretrained(base_model).to(self.device)\n",
    "        \n",
    "        if antiexpert_model:\n",
    "            self.antiexpert = T5ForConditionalGeneration.from_pretrained(antiexpert_model).to(self.device)\n",
    "        else:\n",
    "            self.antiexpert = None\n",
    "        \n",
    "        if expert_model:\n",
    "            self.expert = T5ForConditionalGeneration.from_pretrained(expert_model).to(self.device)\n",
    "        else:\n",
    "            self.expert = None\n",
    "        \n",
    "        self.tokenizer = T5TokenizerFast.from_pretrained(base_model)\n",
    "        self.tokenizer.pad_token = self.STOP_TOKEN\n",
    "        assert self.tokenizer.eos_token_id == self.tokenizer.pad_token_id\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'<DExpertsGenerator model_name_or_path=\"{self.model}\">'\n",
    "\n",
    "    def generate(self,\n",
    "                 prompt: Union[str, List[str]],\n",
    "                 max_len: int = 20,\n",
    "                 sample: bool = True,\n",
    "                 filter_p: float = 0.9,\n",
    "                 k: int = 0,\n",
    "                 p: float = 1.0,\n",
    "                 temperature: float = 1.0,\n",
    "                 alpha: float = 0.0\n",
    "                ):\n",
    "        if isinstance(prompt, str):\n",
    "            source = [prompt]\n",
    "        else:\n",
    "            source = prompt\n",
    "            \n",
    "        source_base = [\"paraphrase: \" + x for x in source]\n",
    "        source_expert = [self.expert_prefix + x for x in source]\n",
    "        source_antiexpert = [self.antiexpert_prefix + x for x in source]\n",
    "        \n",
    "        target = []\n",
    "        for x in source:\n",
    "            target.append(\"<pad>\")\n",
    "            \n",
    "        encodings_dict_base = self.tokenizer.batch_encode_plus(source_base, pad_to_max_length=True, return_tensors='pt')\n",
    "        input_ids_base = encodings_dict_base['input_ids'].to(self.device)\n",
    "        attention_mask_base = encodings_dict_base['attention_mask'].to(self.device)\n",
    "        \n",
    "        encodings_dict_expert = self.tokenizer.batch_encode_plus(source_expert, pad_to_max_length=True, return_tensors='pt')\n",
    "        input_ids_exper = encodings_dict_expert['input_ids'].to(self.device)\n",
    "        attention_mask_exper = encodings_dict_expert['attention_mask'].to(self.device)\n",
    "        \n",
    "        \n",
    "        encodings_dict_anti = self.tokenizer.batch_encode_plus(source_antiexpert, pad_to_max_length=True, return_tensors='pt')\n",
    "        input_ids_anti = encodings_dict_anti['input_ids'].to(self.device)\n",
    "        attention_mask_anti = encodings_dict_anti['attention_mask'].to(self.device)\n",
    "        \n",
    "        decoder_dict = self.tokenizer.batch_encode_plus(target, return_tensors='pt')\n",
    "        decoder_input_ids = decoder_dict['input_ids'].to(self.device)\n",
    "        decoder_attention_mask = decoder_dict['attention_mask'].to(self.device)\n",
    "        \n",
    "        batch_size, input_seq_len = input_ids_exper.shape\n",
    "\n",
    "#         position_ids = attention_mask.cumsum(dim=1) - 1\n",
    "        unfinished_sents = torch.ones(batch_size, dtype=torch.long, device=self.device)\n",
    "\n",
    "        self.base_model.eval()\n",
    "        if self.expert:\n",
    "            self.expert.eval()\n",
    "        if self.antiexpert:\n",
    "            self.antiexpert.eval()\n",
    "        with torch.no_grad():\n",
    "            for step in range(max_len):\n",
    "                # base model prediction\n",
    "                base_logits = self.base_model(input_ids_base, attention_mask = attention_mask_base, \n",
    "                                              decoder_input_ids = decoder_input_ids, decoder_attention_mask = decoder_attention_mask)[\"logits\"]\n",
    "                \n",
    "                # expert prediction\n",
    "                if self.expert:\n",
    "                    expert_logits = self.expert(input_ids_exper, attention_mask = attention_mask_exper, \n",
    "                                               decoder_input_ids = decoder_input_ids, decoder_attention_mask = decoder_attention_mask)[\"logits\"]\n",
    "                else:\n",
    "                    expert_logits = base_logits\n",
    "                \n",
    "                # antiexpert prediction\n",
    "                if self.antiexpert:\n",
    "                    antiexpert_logits = self.antiexpert(input_ids_anti, attention_mask = attention_mask_anti, \n",
    "                                                       decoder_input_ids = decoder_input_ids, decoder_attention_mask = decoder_attention_mask)[\"logits\"]\n",
    "                else:\n",
    "                    antiexpert_logits = base_logits\n",
    "                \n",
    "                if filter_p < 1.0:\n",
    "                    base_logits = top_k_top_p_filtering(base_logits, top_p=filter_p)\n",
    "                \n",
    "                # DExperts\n",
    "                alpha = torch.tensor(alpha).to(self.device)\n",
    "                ensemble_logits = base_logits + alpha * (expert_logits - antiexpert_logits)\n",
    "\n",
    "                # in the first decoding step, we want to use the 'real' last position for each sentence\n",
    "#                 if step == 0:\n",
    "#                     last_non_masked_idx = torch.sum(attention_mask, dim=1) - 1\n",
    "#                     next_token_logits = ensemble_logits[range(batch_size), last_non_masked_idx, :]\n",
    "#                 else:\n",
    "                next_token_logits = ensemble_logits[:, -1, :]\n",
    "\n",
    "                if sample:\n",
    "                    # Temperature (higher temperature => more likely to sample low probability tokens)\n",
    "                    if temperature != 1.0:\n",
    "                        next_token_logits = next_token_logits / temperature\n",
    "                    if k > 0 or p < 1.0:\n",
    "                        next_token_logits = top_k_top_p_filtering(next_token_logits, top_k=k, top_p=p)\n",
    "                    # Sample\n",
    "                    probs = F.softmax(next_token_logits, dim=-1)\n",
    "                    next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)\n",
    "                else:\n",
    "                    # Greedy decoding\n",
    "                    next_tokens = torch.argmax(next_token_logits, dim=-1)\n",
    "                # either append a padding token here if <EOS> has been seen or append next token\n",
    "                tokens_to_add = next_tokens * unfinished_sents + self.tokenizer.pad_token_id * (1 - unfinished_sents)\n",
    "                # this updates which sentences have not seen an EOS token so far\n",
    "                # if one EOS token was seen the sentence is finished\n",
    "                eos_in_sents = tokens_to_add == self.tokenizer.eos_token_id\n",
    "                unfinished_sents.mul_((~eos_in_sents).long())\n",
    "\n",
    "                # stop when there is an EOS in each sentence\n",
    "                if unfinished_sents.max() == 0:\n",
    "                    break\n",
    "                \n",
    "#                 if step == 0:\n",
    "#                     decoder_input_ids = tokens_to_add.unsqueeze(-1)\n",
    "                    \n",
    "#                 else:\n",
    "                # Update input_ids, attention_mask and position_ids\n",
    "                decoder_input_ids = torch.cat([decoder_input_ids, tokens_to_add.unsqueeze(-1)], dim=-1)\n",
    "#                     print(decoder_input_ids)\n",
    "                decoder_attention_mask = torch.cat([decoder_attention_mask, decoder_attention_mask.new_ones((batch_size, 1))], dim=1)\n",
    "#                     position_ids = torch.cat([position_ids, (position_ids[:, -1] + 1).unsqueeze(-1)], dim=1)\n",
    "\n",
    "        decoded_outputs = [self.tokenizer.decode(output, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "                           for output in decoder_input_ids[:, :]]\n",
    "        return decoded_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = DExpertsGeneration(\n",
    "        base_model=\"../st5-para/\", \n",
    "        expert_model=\"../distributed/st5_mul_joy/checkpoint-93330/\",\n",
    "        antiexpert_model=\"../distributed/st5_mul_sad/checkpoint-101983/\",\n",
    "        expert_prefix = \"joy_CLS: \",\n",
    "        antiexpert_prefix = \"sad_CLS: \"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chiyu94/py36/lib/python3.6/site-packages/ipykernel_launcher.py:117: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Who won?',\n",
       " \"it's no longer high school for some people to grow up, not a kind, ladaptive behaviour.\",\n",
       " 'When a 9-year-old becomes a gunman and their parents laugh together at the same time.',\n",
       " \"I'm gonna die of my phone, but the energy will be safe for all four hours.\",\n",
       " 'Why do some secrets have to end so quickly?']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.generate(\n",
    "         [\"And just like that , I don't care who wins the tournament .\", \n",
    "          \"Some people still need to learn to grow up and mature . . This isn't high school anymore\",\n",
    "          \"It's kinda sad when you see an 9 year-old drop the f-bomb in public and the parents laugh along with them.\",\n",
    "          \"My phone is dying yet it's my entertainment for the next 4 hours\", \"why did secret life have to end like that noo ) :\"],\n",
    "         max_len = 128,\n",
    "         sample = True,\n",
    "         filter_p = 0.9,\n",
    "         k = 0,\n",
    "         p = 0.9,\n",
    "         temperature = 1.0,\n",
    "         alpha = 3.0\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokener = T5TokenizerFast.from_pretrained(\"../st5-para/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokener.pad_token = \"</s>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(tokener.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = T5ForConditionalGeneration.from_pretrained(\"../distributed/st5_mul_joy/checkpoint-93330/\", pad_token_id=tokener.eos_token_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings_dict = tokener.batch_encode_plus([\"joy: I hate my day.\"], pad_to_max_length=True, return_tensors='pt')\n",
    "decoder_dict = tokener.batch_encode_plus([\"<pad>I absolutely\"], add_special_tokens=False, return_tensors='pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = encodings_dict['input_ids']\n",
    "attention_mask = encodings_dict['attention_mask']\n",
    "batch_size, input_seq_len = input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input_ids = decoder_dict['input_ids']\n",
    "decoder_attention_mask = decoder_dict['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> I absolutely\n",
      "tensor([[   0,    7, 2776]])\n"
     ]
    }
   ],
   "source": [
    "print(tokener.decode(decoder_input_ids[0].tolist()))\n",
    "print(decoder_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokener.decode([7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_logits = base_model(input_ids, attention_mask= attention_mask,\n",
    "                         decoder_input_ids = decoder_input_ids, decoder_attention_mask = decoder_attention_mask)[\"logits\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 110080])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "next_token_logits = base_logits[:, -1, :]\n",
    "temperature = 1\n",
    "p = 0.9\n",
    "k = 0\n",
    "# Temperature (higher temperature => more likely to sample low probability tokens)\n",
    "if temperature != 1.0:\n",
    "    next_token_logits = next_token_logits / temperature\n",
    "if k > 0 or p < 1.0:\n",
    "    next_token_logits = top_k_top_p_filtering(next_token_logits, top_k=k, top_p=p)\n",
    "    # Sample\n",
    "    probs = F.softmax(next_token_logits, dim=-1)\n",
    "    next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hate'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokener.decode(next_tokens.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I absolutely'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "probabilities, predicted = torch.max(base_logits[0].cpu().data, 1)\n",
    "tokener.decode(predicted.data.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 1,  ..., 0, 1, 1]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
