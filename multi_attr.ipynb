{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Union, List\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, modeling_utils, GPT2PreTrainedModel\n",
    "from generation.gpt2_generation import GPT2Generation\n",
    "\n",
    "from utils import utils\n",
    "from utils.generation_utils import top_k_top_p_filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = int(10000)  # Hardcoded max length to avoid infinite loop\n",
    "\n",
    "class DExpertsGeneration(GPT2Generation): \n",
    "    STOP_TOKEN = \"<|endoftext|>\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        base_model: Union[str, Path, GPT2PreTrainedModel],\n",
    "        antiexpert_model_a1: Union[str, Path, GPT2PreTrainedModel] = None,\n",
    "        expert_model_a1: Union[str, Path, GPT2PreTrainedModel] = None,\n",
    "        antiexpert_model_a2: Union[str, Path, GPT2PreTrainedModel] = None,\n",
    "        expert_model_a2: Union[str, Path, GPT2PreTrainedModel] = None,\n",
    "        tokenizer: str = 'gpt2', \n",
    "        seed: int = 42,\n",
    "    ):\n",
    "        # Set up device\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        n_gpu = torch.cuda.device_count()\n",
    "        utils.set_seed(seed, n_gpu)\n",
    "\n",
    "        self.base_model = GPT2LMHeadModel.from_pretrained(base_model).to(self.device)\n",
    "        \n",
    "        if antiexpert_model_a1:\n",
    "            self.antiexpert1 = GPT2LMHeadModel.from_pretrained(antiexpert_model_a1).to(self.device)\n",
    "        else:\n",
    "            self.antiexpert1 = None\n",
    "        \n",
    "        if expert_model_a1:\n",
    "            self.expert1 = GPT2LMHeadModel.from_pretrained(expert_model_a1).to(self.device)\n",
    "        else:\n",
    "            self.expert1 = None\n",
    "            \n",
    "        if antiexpert_model_a2:\n",
    "            self.antiexpert2 = GPT2LMHeadModel.from_pretrained(antiexpert_model_a2).to(self.device)\n",
    "        else:\n",
    "            self.antiexpert2 = None\n",
    "        \n",
    "        if expert_model_a2:\n",
    "            self.expert2 = GPT2LMHeadModel.from_pretrained(expert_model_a2).to(self.device)\n",
    "        else:\n",
    "            self.expert2 = None\n",
    "        \n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(tokenizer, pad_token=self.STOP_TOKEN)\n",
    "        assert self.tokenizer.eos_token_id == self.tokenizer.pad_token_id\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'<DExpertsGenerator model_name_or_path=\"{self.model}\">'\n",
    "\n",
    "    def generate(self,\n",
    "                 prompt: Union[str, List[str]],\n",
    "                 max_len: int = 20,\n",
    "                 sample: bool = True,\n",
    "                 filter_p: float = 0.9,\n",
    "                 k: int = 0,\n",
    "                 p: float = 1.0,\n",
    "                 temperature: float = 1.0,\n",
    "                 alpha: float = 0.0,\n",
    "                 **model_kwargs):\n",
    "        if isinstance(prompt, str):\n",
    "            prompt = [prompt]\n",
    "\n",
    "        encodings_dict = self.tokenizer.batch_encode_plus(prompt, pad_to_max_length=True, return_tensors='pt')\n",
    "\n",
    "        input_ids = encodings_dict['input_ids'].to(self.device)\n",
    "        attention_mask = encodings_dict['attention_mask'].to(self.device)\n",
    "        batch_size, input_seq_len = input_ids.shape\n",
    "\n",
    "        position_ids = attention_mask.cumsum(dim=1) - 1\n",
    "        unfinished_sents = torch.ones(batch_size, dtype=torch.long, device=self.device)\n",
    "\n",
    "        self.base_model.eval()\n",
    "        self.expert1.eval()\n",
    "        self.antiexpert1.eval()\n",
    "        self.expert2.eval()\n",
    "        self.antiexpert2.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for step in range(max_len):\n",
    "                # base model prediction\n",
    "                base_logits, base_past = self.base_model(\n",
    "                    input_ids, attention_mask=attention_mask, position_ids=position_ids, **model_kwargs)\n",
    "                \n",
    "                # expert prediction\n",
    "                if self.expert1:\n",
    "                    expert_logits1 = self.expert1(\n",
    "                        input_ids, attention_mask=attention_mask, position_ids=position_ids, **model_kwargs)[\"logits\"]\n",
    "                else:\n",
    "                    expert_logits1 = base_logits\n",
    "                \n",
    "                # antiexpert prediction\n",
    "                if self.antiexpert1:\n",
    "                    antiexpert_logits1 = self.antiexpert1(\n",
    "                        input_ids, attention_mask=attention_mask, position_ids=position_ids, **model_kwargs)[\"logits\"]\n",
    "                else:\n",
    "                    antiexpert_logits1 = base_logits\n",
    "                \n",
    "                expert_logits2 = self.expert2(\n",
    "                        input_ids, attention_mask=attention_mask, position_ids=position_ids, **model_kwargs)[\"logits\"]\n",
    "                \n",
    "                antiexpert_logits2 = self.antiexpert2(\n",
    "                        input_ids, attention_mask=attention_mask, position_ids=position_ids, **model_kwargs)[\"logits\"]\n",
    "                \n",
    "                \n",
    "                if filter_p < 1.0:\n",
    "                    base_logits = top_k_top_p_filtering(base_logits, top_p=filter_p)\n",
    "                \n",
    "                # DExperts\n",
    "                alpha = torch.tensor(alpha).to(self.device)\n",
    "                ensemble_logits = base_logits + alpha * (expert_logits1 - antiexpert_logits1) + alpha * (expert_logits2 - antiexpert_logits2)\n",
    "                \n",
    "\n",
    "                # in the first decoding step, we want to use the 'real' last position for each sentence\n",
    "                if step == 0:\n",
    "                    last_non_masked_idx = torch.sum(attention_mask, dim=1) - 1\n",
    "                    next_token_logits = ensemble_logits[range(batch_size), last_non_masked_idx, :]\n",
    "                else:\n",
    "                    next_token_logits = ensemble_logits[:, -1, :]\n",
    "\n",
    "                if sample:\n",
    "                    # Temperature (higher temperature => more likely to sample low probability tokens)\n",
    "                    if temperature != 1.0:\n",
    "                        next_token_logits = next_token_logits / temperature\n",
    "                    if k > 0 or p < 1.0:\n",
    "                        next_token_logits = top_k_top_p_filtering(next_token_logits, top_k=k, top_p=p)\n",
    "                    # Sample\n",
    "                    probs = F.softmax(next_token_logits, dim=-1)\n",
    "                    next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)\n",
    "                else:\n",
    "                    # Greedy decoding\n",
    "                    next_tokens = torch.argmax(next_token_logits, dim=-1)\n",
    "\n",
    "                # either append a padding token here if <EOS> has been seen or append next token\n",
    "                tokens_to_add = next_tokens * unfinished_sents + self.tokenizer.pad_token_id * (1 - unfinished_sents)\n",
    "\n",
    "                # this updates which sentences have not seen an EOS token so far\n",
    "                # if one EOS token was seen the sentence is finished\n",
    "                eos_in_sents = tokens_to_add == self.tokenizer.eos_token_id\n",
    "                unfinished_sents.mul_((~eos_in_sents).long())\n",
    "\n",
    "                # stop when there is an EOS in each sentence\n",
    "                if unfinished_sents.max() == 0:\n",
    "                    break\n",
    "\n",
    "                # Update input_ids, attention_mask and position_ids\n",
    "                input_ids = torch.cat([input_ids, tokens_to_add.unsqueeze(-1)], dim=-1)\n",
    "                attention_mask = torch.cat([attention_mask, attention_mask.new_ones((batch_size, 1))], dim=1)\n",
    "                position_ids = torch.cat([position_ids, (position_ids[:, -1] + 1).unsqueeze(-1)], dim=1)\n",
    "\n",
    "        decoded_outputs = [self.tokenizer.decode(output, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "                           for output in input_ids[:, input_seq_len:]]\n",
    "        return decoded_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = DExpertsGeneration(\n",
    "        base_model=\"/home/chiyu94/scratch/hashtag_paraphrase/gpt2-large\", \n",
    "        expert_model_a1=\"/home/chiyu94/scratch/hashtag_paraphrase/formality/fine-tuned/informal/checkpoint-9000\",\n",
    "        antiexpert_model_a1=\"/home/chiyu94/scratch/hashtag_paraphrase/formality/fine-tuned/informal/checkpoint-9000\",\n",
    "        expert_model_a2=\"/home/chiyu94/scratch/dexperts/models/experts/sentiment/large/finetuned_gpt2_positive\",\n",
    "        antiexpert_model_a2=\"/home/chiyu94/scratch/dexperts/models/experts/sentiment/large/finetuned_gpt2_negative\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
